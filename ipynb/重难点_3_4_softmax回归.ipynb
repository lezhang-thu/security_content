{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# softmax回归\n",
    "\n",
    "## 分类问题\n",
    "\n",
    "重难点\\_Softmax回归.pdf\n",
    "\n",
    "假设每次输入是一个$2\\times2$的灰度图像。\n",
    "我们可以用一个标量表示每个像素值，每个图像对应四个特征$x_1, x_2, x_3, x_4$。\n",
    "此外，假设每个图像属于类别“猫”，“鸡”和“狗”中的一个。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "接下来，我们要选择如何表示标签。\n",
    "\n",
    "独热编码是一个向量，它的分量和类别一样多。\n",
    "类别对应的分量设置为1，其他所有分量设置为0。\n",
    "\n",
    "$(1, 0, 0)$对应于“猫”、$(0, 1, 0)$对应于“鸡”、$(0, 0, 1)$对应于“狗”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 网络架构\n",
    "\n",
    "为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。\n",
    "\n",
    "$$\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"softmaxreg.svg\" width=\"50%\"></center>\n",
    "<center>softmax回归是一种单层神经网络</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## softmax运算\n",
    "\n",
    "softmax运算不会改变未规范化的预测$\\mathbf{o}$之间的顺序，只会确定分配给每个类别的概率。\n",
    "因此，在预测过程中，我们仍然可以用下式来选择最有可能的类别。\n",
    "\n",
    "$$\n",
    "\\operatorname*{argmax}_j \\hat y_j = \\operatorname*{argmax}_j o_j.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。\n",
    "\n",
    "因此，softmax回归是一个*线性模型*（linear model）。\n",
    "\n",
    "（课堂提问：为什么softmax回归是一个线性模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### softmax回归为线性模型的分析\n",
    "\n",
    "考虑softmax在类别个数为2时的情形\n",
    "\n",
    "**logistic sigmoid**\n",
    "\n",
    "$$\n",
    "p=\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}}}\n",
    "$$\n",
    "\n",
    "decision boundary为$p=0.5$\n",
    "\n",
    "因此得到：$\\mathbf{w}^T\\mathbf{x}=0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 小批量样本的矢量化\n",
    "\n",
    "假设我们读取了一个批量的样本$\\mathbf{X}$，\n",
    "其中特征维度（输入数量）为$d$，批量大小为$n$。\n",
    "此外，假设我们在输出中有$q$个类别。\n",
    "那么小批量特征为$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，\n",
    "权重为$\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$，\n",
    "偏置为$\\mathbf{b} \\in \\mathbb{R}^{1\\times q}$。\n",
    "softmax回归的矢量计算表达式为：\n",
    "\n",
    "$$ \\begin{aligned} \\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\\\ \\hat{\\mathbf{Y}} & = \\mathrm{softmax}(\\mathbf{O}). \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 损失函数\n",
    "\n",
    "### 对数似然\n",
    "\n",
    "假设整个数据集$\\{\\mathbf{X}, \\mathbf{Y}\\}$具有$n$个样本，\n",
    "其中索引$i$的样本由特征向量$\\mathbf{x}^{(i)}$和独热标签向量$\\mathbf{y}^{(i)}$组成。\n",
    "\n",
    "$$\n",
    "P(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "根据最大似然估计，我们最大化$P(\\mathbf{Y} \\mid \\mathbf{X})$，相当于最小化负对数似然：\n",
    "\n",
    "$$\n",
    "-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n",
    "= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),\n",
    "$$\n",
    "\n",
    "其中，对于任何标签$\\mathbf{y}$和模型预测$\\hat{\\mathbf{y}}$，损失函数为：\n",
    "\n",
    "$$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "由于$\\mathbf{y}$是一个长度为$q$的独热编码向量，\n",
    "所以除了一个项以外的所有项$j$都消失了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### softmax及其导数\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l(\\mathbf{y}, \\hat{\\mathbf{y}}) &=  - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\\n",
    "&= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j\\\\\n",
    "&= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "考虑相对于任何未规范化的预测$o_j$的导数，我们得到：\n",
    "\n",
    "$$\n",
    "\\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "换句话说，导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。\n",
    "从这个意义上讲，这与我们在回归中看到的非常相似，\n",
    "其中梯度是观测值$y$和估计值$\\hat{y}$之间的差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 损失函数\n",
    "\n",
    "[https://www.bilibili.com/video/BV1K64y1Q7wu?p=2](https://www.bilibili.com/video/BV1K64y1Q7wu?p=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 我们可以更深入地探讨指数族与softmax之间的联系。\n",
    "    1. 计算softmax交叉熵损失$l(\\mathbf{y},\\hat{\\mathbf{y}})$的二阶导数。\n",
    "    1. 计算$\\mathrm{softmax}(\\mathbf{o})$给出的分布方差，并与上面计算的二阶导数匹配。\n",
    "1. 假设我们有三个类发生的概率相等，即概率向量是$(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$。\n",
    "    1. 如果我们尝试为它设计二进制代码，有什么问题？\n",
    "    1. 你能设计一个更好的代码吗？提示：如果我们尝试编码两个独立的观察结果会发生什么？如果我们联合编码$n$个观测值怎么办？\n",
    "1. softmax是对上面介绍的映射的误称（虽然深度学习领域中很多人都使用这个名字）。真正的softmax被定义为$\\mathrm{RealSoftMax}(a, b) = \\log (\\exp(a) + \\exp(b))$。\n",
    "    1. 证明$\\mathrm{RealSoftMax}(a, b) > \\mathrm{max}(a, b)$。\n",
    "    1. 证明$\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) > \\mathrm{max}(a, b)$成立，前提是$\\lambda > 0$。\n",
    "    1. 证明对于$\\lambda \\to \\infty$，有$\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) \\to \\mathrm{max}(a, b)$。\n",
    "    1. soft-min会是什么样子？\n",
    "    1. 将其扩展到两个以上的数字。"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
