{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 长短期记忆网络（LSTM）\n",
    "\n",
    "* long-term information preservation\n",
    "* short-term input skipping\n",
    "\n",
    "有趣的是，长短期记忆网络的设计比门控循环单元稍微复杂一些， 却比门控循环单元早诞生了近20年。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 门控记忆元\n",
    "\n",
    "*记忆元*（memory cell）\n",
    "\n",
    "* 与隐状态具有相同的形状\n",
    "* engineered to record additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 输入门、忘记门和输出门\n",
    "\n",
    "<center><img src=\"../img/lstm-0.svg\" width=\"40%\"></center>\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\\n",
    "\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f),\\\\\n",
    "\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o),\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 候选记忆元\n",
    "\n",
    "*候选记忆元*（candidate memory cell）\n",
    "\n",
    "<center><img src=\"../img/lstm-1.svg\" width=\"40%\"></center>\n",
    "\n",
    "$$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 记忆元\n",
    "\n",
    "<center><img src=\"../img/lstm-2.svg\" width=\"40%\"></center>\n",
    "\n",
    "$$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$$\n",
    "\n",
    "* 输入门$\\mathbf{I}_t$控制采用多少来自$\\tilde{\\mathbf{C}}_t$的新数据\n",
    "* 遗忘门$\\mathbf{F}_t$控制保留多少过去的记忆元$\\mathbf{C}_{t-1}$的内容\n",
    "* to alleviate the vanishing gradient problem\n",
    "* to better capture long range dependencies within sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "### 隐状态\n",
    "\n",
    "<center><img src=\"../img/lstm-3.svg\" width=\"40%\"></center>\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t)$$\n",
    "\n",
    "* In LSTM it is simply a gated version of the $\\tanh$ of the memory cell\n",
    "* 输出门接近$0$：retain all the information only within the memory cell and perform no further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”\n",
    "* 只有隐状态会传递到输出层，而记忆元完全属于内部信息\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "\n",
    "1. 既然候选记忆元通过使用$\\tanh$函数来确保值范围在$(-1,1)$之间，那么为什么隐状态需要再次使用$\\tanh$函数来确保输出值范围在$(-1,1)$之间呢？\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
