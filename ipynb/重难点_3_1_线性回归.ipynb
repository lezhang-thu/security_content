{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 线性回归\n",
    "\n",
    "*回归*（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。\n",
    "在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "在机器学习领域中的大多数任务通常都与*预测*（prediction）有关。\n",
    "当我们想预测一个数值时，就会涉及到回归问题。\n",
    "常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、\n",
    "预测需求（零售销量等）。\n",
    "但不是所有的*预测*都是回归问题。\n",
    "在后面的章节中，我们将介绍分类问题。分类问题的目标是预测数据属于一组类别中的哪一个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 线性回归的基本元素\n",
    "\n",
    "*线性回归*（linear regression）基于几个简单的假设：\n",
    "首先，假设自变量$\\mathbf{x}$和因变量$y$之间的关系是线性的，\n",
    "即$y$可以表示为$\\mathbf{x}$中元素的加权和，这里通常允许包含观测值的一些噪声；\n",
    "其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "对于特征集合$\\mathbf{X}$，预测值$\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$\n",
    "可以通过矩阵-向量乘法表示为：\n",
    "\n",
    "$${\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "虽然我们相信给定$\\mathbf{x}$预测$y$的最佳模型会是线性的，\n",
    "但我们很难找到一个有$n$个样本的真实数据集，其中对于所有的$1 \\leq i \\leq n$，$y^{(i)}$完全等于$\\mathbf{w}^T \\mathbf{x}^{(i)}+b$。\n",
    "无论我们使用什么手段来观察特征$\\mathbf{X}$和标签$\\mathbf{y}$，\n",
    "都可能会出现少量的观测误差。\n",
    "因此，即使确信特征与标签的潜在关系是线性的，\n",
    "我们也会加入一个噪声项来考虑观测误差带来的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 损失函数\n",
    "\n",
    "为了度量模型在整个数据集上的质量，我们需计算在训练集$n$个样本上的损失均值（也等价于求和）。\n",
    "\n",
    "$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^T \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "在训练模型时，我们希望寻找一组参数（$\\mathbf{w}^*, b^*$），\n",
    "这组参数能最小化在所有训练样本上的总损失。如下式：\n",
    "\n",
    "$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 解析解\n",
    "\n",
    "首先，我们将偏置$b$合并到参数$\\mathbf{w}$中，合并方法是在$\\mathbf{X}$矩阵中附加一列。\n",
    "\n",
    "（课堂提问：怎样合并）\n",
    "\n",
    "将损失关于$\\mathbf{w}$的导数设为0，得到解析解：\n",
    "\n",
    "$$\\mathbf{w}^* = (\\mathbf X^T \\mathbf X)^{-1}\\mathbf X^T \\mathbf{y}.$$\n",
    "\n",
    "参考：重难点\\_线性回归.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 随机梯度下降\n",
    "\n",
    "1. 初始化模型参数的值，如随机初始化\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤\n",
    "\n",
    "$$\\begin{aligned} \\mathbf{w} &\\leftarrow \\mathbf{w} -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^T \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\\\\ b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_b l^{(i)}(\\mathbf{w}, b)  = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^T \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 正态分布与平方损失\n",
    "\n",
    "正态分布和线性回归之间的关系很密切。\n",
    "\n",
    "简单的说，若随机变量$x$具有均值$\\mu$和方差$\\sigma^2$（标准差$\\sigma$），其正态分布概率密度函数如下：\n",
    "\n",
    "$$p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x - \\mu)^2\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是：\n",
    "我们假设了观测中包含噪声，其中噪声服从正态分布。\n",
    "噪声正态分布如下式:\n",
    "\n",
    "$$y = \\mathbf{w}^T \\mathbf{x} + b + \\epsilon,$$\n",
    "\n",
    "其中，$\\epsilon \\sim N(0, \\sigma^2)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "因此，我们现在可以写出通过给定的$\\mathbf{x}$观测到特定$y$的*似然*（likelihood）：\n",
    "\n",
    "$$P(y \\mid \\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^T \\mathbf{x} - b)^2\\right).$$\n",
    "\n",
    "现在，根据极大似然估计法，参数$\\mathbf{w}$和$b$的最优值是使整个数据集的*似然*最大的值：\n",
    "\n",
    "$$P(\\mathbf y \\mid \\mathbf X) = \\prod_{i=1}^{n} p(y^{(i)}|\\mathbf{x}^{(i)}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "我们可以改为*最小化负对数似然*$-\\log P(\\mathbf y \\mid \\mathbf X)$。\n",
    "由此可以得到的数学公式是：\n",
    "\n",
    "$$-\\log P(\\mathbf y \\mid \\mathbf X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b\\right)^2.$$\n",
    "\n",
    "现在我们只需要假设$\\sigma$是某个固定常数就可以忽略第一项，\n",
    "因为第一项不依赖于$\\mathbf{w}$和$b$。\n",
    "现在第二项除了常数$\\frac{1}{\\sigma^2}$外，其余部分和前面介绍的均方误差是一样的。\n",
    "幸运的是，上面式子的解并不依赖于$\\sigma$。\n",
    "因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 神经网络图\n",
    "\n",
    "<center><img src=\"singleneuron.svg\" width=\"50%\"></center>\n",
    "<center>线性回归是一个单层神经网络</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 当今大多数深度学习的研究几乎没有直接从神经科学中获得灵感。\n",
    "\n",
    "* 虽然飞机可能受到鸟类的启发，但几个世纪以来，鸟类学并不是航空创新的主要驱动力。\n",
    "\n",
    "* 同样地，如今在深度学习中的灵感同样或更多地来自数学、统计学和计算机科学。"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
