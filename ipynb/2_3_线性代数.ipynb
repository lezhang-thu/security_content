{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 92,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 点积（Dot Product）\n",
    "\n",
    "给定两个向量$\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^d$，\n",
    "它们的*点积*（dot product）$\\mathbf{x}^T\\mathbf{y}$\n",
    "（或$\\langle\\mathbf{x},\\mathbf{y}\\rangle$）\n",
    "是相同位置的按元素乘积的和：$\\mathbf{x}^T \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 94,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "y = torch.ones(4, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "origin_pos": 98,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 当权重为非负数且和为1（即$\\sum_{i=1}^{d}{w_i}=1$）时，\n",
    "点积表示*加权平均*（weighted average）\n",
    "* 将两个向量规范化得到单位长度后，点积表示它们夹角的余弦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 100,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 矩阵-向量积\n",
    "\n",
    "让我们将矩阵$\\mathbf{A}$用它的行向量表示：\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_m \\\\\n",
    "\\end{bmatrix},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "矩阵向量积$\\mathbf{A}\\mathbf{x}$是一个长度为$m$的列向量，\n",
    "其第$i$个元素是点积$\\mathbf{a}^T_i \\mathbf{x}$：\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^T_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^T_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^T_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "我们可以把一个矩阵$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$乘法看作是一个从$\\mathbb{R}^{n}$到$\\mathbb{R}^{m}$向量的转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 102,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "当我们为矩阵`A`和向量`x`调用`torch.mv(A, x)`时，会执行矩阵-向量积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 105,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 107,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 矩阵-矩阵乘法\n",
    "\n",
    "假设我们有两个矩阵$\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$和$\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "origin_pos": 109,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 范数（norm）\n",
    "\n",
    "非正式地说，一个向量的*范数*告诉我们一个向量有多大。\n",
    "\n",
    "在线性代数中，向量范数是将向量映射到标量的函数$f$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. $$f(\\mathbf{x}) \\geq 0$$\n",
    "1. $f(\\mathbf{x})=0$ only if $\\mathbf{x}=\\mathbf{0}$\n",
    "1. $$f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x})$$\n",
    "1. （三角不等式）：$$f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 111,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "欧几里得距离是一个$L_2$范数：\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "origin_pos": 113,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 115,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$L_1$范数：\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$\n",
    "\n",
    "（与$L_2$范数相比，$L_1$范数受异常值的影响较小）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "origin_pos": 117,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "更一般的$L_p$范数：\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_p = \\biggl(\\sum_{i=1}^n \\left|x_i \\right|^p \\biggr)^{1/p}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 119,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "矩阵$\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$的*Frobenius范数*（Frobenius norm）：\n",
    "\n",
    "$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$\n",
    "\n",
    "它就像是矩阵形向量的$L_2$范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "origin_pos": 121,
    "slideshow": {
     "slide_type": "-"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 范数和目标\n",
    "\n",
    "1. 在深度学习中，我们经常试图解决优化问题：\n",
    "    1. *最大化*分配给观测数据的概率;\n",
    "    1. *最小化*预测和真实观测之间的距离。\n",
    "1. 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。\n",
    "1. 目标通常被表达为范数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Singular value decomposition\n",
    "\n",
    "SVD\n",
    "\n",
    "* $A\\in\\mathbf{R}^{m\\times n}$，其中$A$的rank为$r$\n",
    "* $A=U\\Sigma V^T$\n",
    "* $U^TU=I$，其中$U\\in \\mathbf{R}^{m\\times r}$\n",
    "* $V^TV=I$，其中$V\\in \\mathbf{R}^{n\\times r}$\n",
    "* $\\Sigma=\\mathbf{diag}(\\sigma_1,\\dotsc,\\sigma_r)$, $\\sigma_1\\ge\\dotsb\\ge \\sigma_r>0$\n",
    "\n",
    "联系：5_线性代数.pdf（特征向量和特征值）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $A\\mathbf{x}=U\\Sigma V^T\\mathbf{x}$\n",
    "* $V^T\\mathbf{x}=\\mathbf{x}'$\n",
    "* $VV^T\\mathbf{x}=V\\mathbf{x}'$也就是$\\mathbf{x}=V\\mathbf{x}'$\n",
    "* $A\\mathbf{x}=U\\Sigma \\mathbf{x}'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"svd.png\" width=\"30%\"></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
