{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 导数和微分\n",
    "\n",
    "我们首先讨论导数的计算，这是几乎所有深度学习优化算法的关键步骤。\n",
    "在深度学习中，我们通常选择对于模型参数可微的损失函数。\n",
    "简而言之，对于每个参数，\n",
    "如果我们把这个参数*增加*或*减少*一个无穷小的量，我们可以知道损失会以多快的速度增加或减少，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "假设我们有一个函数$f: \\mathbb{R} \\rightarrow \\mathbb{R}$，其输入和输出都是标量。\n",
    "如果$f$的*导数*存在，这个极限被定义为\n",
    "\n",
    "$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.$$\n",
    "\n",
    "如果$f'(a)$存在，则称$f$在$a$处是*可微*（differentiable）的。\n",
    "如果$f$在一个区间内的每个数上都是可微的，则此函数在此区间中是可微的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 偏导数\n",
    "\n",
    "在深度学习中，函数通常依赖于许多变量。\n",
    "因此，我们需要将微分的思想推广到*多元函数*（multivariate function）上。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "设$y = f(x_1, x_2, \\ldots, x_n)$是一个具有$n$个变量的函数。\n",
    "$y$关于第$i$个参数$x_i$的*偏导数*（partial derivative）为：\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n",
    "\n",
    "为了计算$\\frac{\\partial y}{\\partial x_i}$，\n",
    "我们可以简单地将$x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$看作常数，\n",
    "并计算$y$关于$x_i$的导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 梯度\n",
    "\n",
    "我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的*梯度*（gradient）向量。\n",
    "具体而言，设函数$f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$的输入是\n",
    "一个$n$维向量$\\mathbf{x}=[x_1,x_2,\\ldots,x_n]^T$，并且输出是一个标量。\n",
    "函数$f(\\mathbf{x})$相对于$\\mathbf{x}$的梯度是一个包含$n$个偏导数的向量:\n",
    "\n",
    "$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^T,$$\n",
    "\n",
    "其中$\\nabla_{\\mathbf{x}} f(\\mathbf{x})$通常在没有歧义时被$\\nabla f(\\mathbf{x})$取代。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivative and gradient\n",
    "\n",
    "the *first-order approximation* of $f$ at (or near) $x$\n",
    "\n",
    "$$\\begin{equation*}\n",
    "    f(x)+Df(x)(z-x)\n",
    "\\end{equation*}$$\n",
    "\n",
    "more on $Df(x)$. for $f:\\mathbb{R}^n\\to\\mathbb{R}^m$. so $f(x)\\in\\mathbb{R}^m$, and $x\\in\\mathbb{R}^n$. So the shape of $Df(x)$ should be $\\mathbb{R}^{m\\times n}$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "if using gradient, the form is\n",
    "$$\\begin{equation*}\n",
    "    f(x)+\\nabla f(x)^T(z-x)\n",
    "\\end{equation*}$$\n",
    "\n",
    "so you see $\\nabla f(x)=Df(x)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "假设$\\mathbf{x}$为$n$维向量，在微分多元函数时经常使用以下规则:\n",
    "\n",
    "* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$，都有$D \\mathbf{A} \\mathbf{x} = \\mathbf{A}$\n",
    "* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$，\n",
    "考虑$\\mathbf{x}^T\\mathbf{A}$，因为其的结果为*行向量*，考虑对其进行转换$(\\mathbf{x}^T\\mathbf{A})^T=\\mathbf{A}^T\\mathbf{x}$，因此有$D \\mathbf{A}^T \\mathbf{x} = \\mathbf{A}^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$，那么$D \\mathbf{x}^T \\mathbf{A} \\mathbf{x}$为多少\n",
    "\n",
    "思路：将$\\mathbf{x}^T \\mathbf{A} \\mathbf{x}$理解为$\\mathbf{x}^T (\\mathbf{A} \\mathbf{x})$，注意\n",
    "$\\mathbf{x}^T (\\mathbf{A} \\mathbf{x})=(\\mathbf{A} \\mathbf{x})^T\\mathbf{x}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "结果为$\\mathbf{x}^T(\\mathbf{A} + \\mathbf{A}^T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^T \\mathbf{x} = 2\\mathbf{x}$\n",
    "\n",
    "同样，对于任何矩阵$\\mathbf{X}$，都有$\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_F^2 = 2\\mathbf{X}$。\n",
    "\n",
    "* 6\\_矩阵计算.pdf的第5页：样例\n",
    "\n",
    "注意$\\langle\\mathbf{u},\\mathbf{v}\\rangle=\\mathbf{u}^T\\mathbf{v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 练习\n",
    "\n",
    "7\\_自动求导.pdf的第2页：例子1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## 练习\n",
    "\n",
    "1. 求函数$f(\\mathbf{x}) = 3x_1^2 + 5e^{x_2}$的梯度。\n",
    "1. 函数$f(\\mathbf{x}) = \\|\\mathbf{x}\\|_2$的梯度是什么？\n",
    "\n",
    "提示：考虑$y=\\|\\mathbf{x}\\|^2$，所以，$f(\\mathbf{x})=\\sqrt{y}$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
