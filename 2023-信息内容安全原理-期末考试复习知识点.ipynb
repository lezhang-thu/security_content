{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47df9cc",
   "metadata": {},
   "source": [
    "1. 你在设计一个卷积神经网络。对于每一层，请计算weights的个数，biases的个数，以及相应的特征图的大小。\n",
    "   \n",
    "   相关符号的含义如下：\n",
    "   * CONV-K-N表示卷积核的个数为N，每个卷积核的大小为KxK，填充和步幅分别为0和1；\n",
    "   * POOL-K表示一个KxK的池化层，步幅为K，填充为0；\n",
    "   * FC-N表示一个全连接层，有N个神经元。\n",
    "       \n",
    "| Layer     | Activation map dimensions  | Number of weights  | Number of biases  |\n",
    "| :----:    |  :-----------------------: | :----------------: | :--------------:  |\n",
    "| INPUT     |  128x128x3                 |         0          |           0       |\n",
    "| CONV-9-32 |                            |                    |                   |\n",
    "| POOL-2    |                            |                    |                   |\n",
    "| CONV-5-64 |                            |                    |                   |\n",
    "| POOL-2    |                            |                    |                   |\n",
    "| CONV-5-64 |                            |                    |                   |\n",
    "| POOL-2    |                            |                    |                   |\n",
    "| FC-3      |                            |                    |                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff403274",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "答：\n",
    "* 输出形状为：$H'=\\lfloor\\tfrac{H-K+2P}{S}\\rfloor+1=\\lfloor\\tfrac{H-K}{1}\\rfloor+1=H-K+1$；#weights为$N\\times K\\times K\\times C$，其中$C$为输入通道数，而#biases为$N$\n",
    "* 注意池化层没有参数，而输出形状为：$H'=\\lfloor\\tfrac{H-K+2P}{S}\\rfloor+1=\\lfloor\\tfrac{H-K}{K}\\rfloor+1=\\lfloor\\tfrac{H}{K}\\rfloor$\n",
    "* 输出形状为：$N$。#weights为$M\\times N$，其中$M$为输入向量的长度，而#biases为$N$\n",
    "\n",
    "| Layer     | Activation map dimensions  | Number of weights  | Number of biases  |\n",
    "| :----:    |  :-----------------------: | :----------------: | :--------------:  |\n",
    "| INPUT     |  128x128x3                 |         0          |           0       |\n",
    "| CONV-9-32 |  120x120x32                |      32x9x9x3      |          32       |\n",
    "| POOL-2    |  60x60x32                  |      0             |          0        |\n",
    "| CONV-5-64 |  56x56x64                  |      64x5x5x32     |  64               |\n",
    "| POOL-2    |  28x28x64                  |      0             |   0               |\n",
    "| CONV-5-64 |  24x24x64                  |      64x5x5x64     |   64              |\n",
    "| POOL-2    |  12x12x64                  |      0             |  0                |\n",
    "| FC-3      |  3                         |     3x12x12x64     |   3               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae4b35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. 逻辑回归 / Logistic Regression\n",
    "\n",
    "给定如下的逻辑回归，其中$(x_1,x_2)$为输入，$y$为相应的标签值（为0或者1）。\n",
    "\n",
    "计算过程如下：\n",
    "\n",
    "\\begin{align*}\n",
    "z&=w_1x_1+w_2x_2+b\\\\\n",
    "a&=\\sigma(z)\n",
    "\\end{align*}\n",
    "\n",
    "其中，$\\sigma(z)=\\tfrac{1}{1+e^{-z}}$，同时，$L(a,y)=-(y\\log a+(1-y)\\log (1-a))$，其中，$\\log$以自然底数$e$为底。\n",
    "\n",
    "请回答如下的问题：\n",
    "\n",
    "a. 请根据链式法则，给出$\\tfrac{dL(a,y)}{dz}$的计算过程；\n",
    "\n",
    "b. 若采取的是随机梯度下降算法，且学习率为1e-4，那么，在$(w_1,w_2,b)$为$(0.5,-0.5,0)$时，若$(x_1,x_2,y)$为$(1,1,0)$，则执行一次梯度下降迭代后，$(w_1,w_2,b)$为多少。请给出计算过程。\n",
    "\n",
    "答：\n",
    "\n",
    "a.\n",
    "\n",
    "因为$\\tfrac{dL(a,y)}{dz}=\\tfrac{dL(a,y)}{da}\\tfrac{da}{dz}$，这里先求解$\\tfrac{da}{dz}$，有如下公式：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{da}{dz}&=\\frac{d\\sigma(z)}{dz}\\\\\n",
    "&=(-1)(1+e^{-z})^{-2}e^{-z}\\\\\n",
    "&=\\sigma(z)(1-\\sigma(z))\\\\\n",
    "&=a(1-a)\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "而对于$\\tfrac{dL(a,y)}{da}$，有：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{dL(a,y)}{da}=-\\frac{y}{a}+\\frac{1-y}{1-a}\n",
    "\\end{equation*}\n",
    "\n",
    "综上，有：$\\tfrac{dL(a,y)}{dz}=a-y$。\n",
    "\n",
    "b.\n",
    "\n",
    "因为采取的是随机梯度下降算法，所以，需要计算如$\\tfrac{\\partial L(a,y)}{\\partial w_1}$等的值。现在已经知道$\\tfrac{dL(a,y)}{dz}=a-y$，而因为$y=0$，且$z=0.5\\cdot 1-0.5\\cdot 1+0=0$，所以$a=\\sigma(z)=0.5$，$\\tfrac{dL(a,y)}{dz}=0.5$。\n",
    "\n",
    "这样的话，$\\tfrac{\\partial L(a,y)}{\\partial w_1}=0.5x_1=0.5,\\tfrac{\\partial L(a,y)}{\\partial w_2}=0.5x_2=0.5$，同时也有：$\\tfrac{\\partial L(a,y)}{\\partial b}=0.5$。那么，经过一次梯度下降之后，$w_1=0.5-10^{-4}\\cdot 0.5,w_2=-0.5-10^{-4}\\cdot 0.5,b=-10^{-4}\\cdot 0.5$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936041a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. 给定一个63x63x16的输入，并使用大小为7x7的32个卷积核进行卷积（步幅为2和无填充），请问输出是多少，为什么。\n",
    "\n",
    "4. 给定输入的张量，形状为64x64x16，请问作用于其上的单个1x1的卷积核含有多少个参数（包括bias），为什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edc436",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "答：\n",
    "\n",
    "3.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "H'&=\\lfloor\\frac{H-K+2P}{S}\\rfloor+1\\\\\n",
    "&=\\lfloor\\frac{63-7+2\\cdot 0}{2}\\rfloor+1\\\\\n",
    "&=29\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "所以，输出的形状为：$29\\times 29\\times 32$。\n",
    "\n",
    "4.\n",
    "\n",
    "weights包括16个参数，因为输入通道数为16；\n",
    "\n",
    "bias包括1个参数，因为考虑的为单个的卷积核，输出通道数因此为1。\n",
    "\n",
    "总的参数个数为17个。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d3263",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "5. 均方损失\n",
    "\n",
    "给定如下的计算过程，其中$(x_1,x_2)$为输入，$y$为目标值：\n",
    "\n",
    "\\begin{align*}\n",
    "z_1&=w_{11}x_1+w_{12}x_2+b_1\\\\\n",
    "z_2&=w_{21}x_1+w_{22}x_2+b_2\\\\\n",
    "a_1&=\\mathrm{ReLU}(z_1)\\\\\n",
    "a_2&=\\mathrm{ReLU}(z_2)\\\\\n",
    "a&=a_1w_{31}+a_2w_{32}\n",
    "\\end{align*}\n",
    "\n",
    "而损失函数为均方损失，也就是说，$L(a,y)=\\tfrac{1}{2}(a-y)^2$。\n",
    "\n",
    "若采取的是随机梯度下降算法，且学习率为1e-4，那么，在$b_1=b_2=0$，$w_{11}=w_{12}=-1$，$w_{21}=w_{22}=1$，$w_{31}=w_{32}=1$时，若$(x_1,x_2,y)$为$(1,1,0.5)$，则执行一次梯度下降迭代后，$w_{21}$为多少。请给出计算过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f66dc62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "答：\n",
    "\n",
    "为执行一次梯度下降迭代，需要计算得到$\\tfrac{\\partial L(a,y)}{\\partial w_{21}}$。\n",
    "\n",
    "计算过程如下：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{\\partial L(a,y)}{\\partial w_{21}}&=\\frac{\\partial L(a,y)}{\\partial a}\\frac{\\partial a}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_2}\\frac{\\partial z_2}{\\partial w_{21}}\\\\\n",
    "&=(a-y)w_{32}\\frac{\\partial a_2}{\\partial z_2}x_1\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "因为$w_{21}=w_{22}=1$，且$b_2=0$，所以，$(x_1,x_2)=(1,1)$时，有：$z_2=1\\cdot 1+1\\cdot 1+0=2>0$，所以，$a_2=z_2=2$。\n",
    "\n",
    "所以，可得到$\\frac{\\partial a_2}{\\partial z_2}=1$。故：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L(a,y)}{\\partial w_{21}}=(a-y)w_{32}\\cdot 1\\cdot x_1\n",
    "\\end{equation*}\n",
    "\n",
    "同时，因为$z_1=-1\\cdot 1-1\\cdot 1+0<0$，所以，$z_1<0$。所以，$a_1=0$。因此，$a=0\\cdot w_{31}+2\\cdot 1=2$。\n",
    "\n",
    "所以，\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L(a,y)}{\\partial w_{21}}=(2-0.5)\\cdot 1\\cdot 1\\cdot 1=1.5\n",
    "\\end{equation*}\n",
    "\n",
    "所以，执行一次梯度下降迭代后，$w_{21}=1-10^{-4}\\cdot 1.5$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e373438a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "6. 给定输入tensor `x`：\n",
    "\n",
    "```\n",
    "tensor([[ 0,  1,  2,  3],\n",
    "        [ 4,  5,  6,  7]])\n",
    "```\n",
    "\n",
    "考虑卷积核`K`（注意bias为0）：\n",
    "\n",
    "```\n",
    "tensor([[-1.,  1.],\n",
    "        [ 1.,  1.]])\n",
    "```\n",
    "\n",
    "a. 若stride为1，padding为0，那么，输出的tensor应该是什么？\n",
    "\n",
    "b. 现在需要你用全连接层去替换掉上述卷积层，并得到和问题a相同的输出tensor。\n",
    "\n",
    "那么，请问全连接层的weights和biases分别是什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e76f7ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "答：\n",
    "\n",
    "a.\n",
    "\n",
    "若stride为1，padding为0，那么，输出的tensor应该是：\n",
    "\n",
    "```\n",
    "tensor([[10., 12., 14.]])\n",
    "```\n",
    "\n",
    "b.\n",
    "\n",
    "注意从全连接层到卷积层，使用了平移不变性和局部性。\n",
    "\n",
    "所以，首先卷积层所能表达的可以被全连接层所表达，只是前者增加了上述两个限制。\n",
    "\n",
    "因此相应的全连接层的weights为：\n",
    "\n",
    "```\n",
    "tensor([[-1.,  1.,  0.,  0.,  1.,  1.,  0.,  0.],\n",
    "        [ 0., -1.,  1.,  0.,  0.,  1.,  1.,  0.],\n",
    "        [ 0.,  0., -1.,  1.,  0.,  0.,  1.,  1.]])\n",
    "```\n",
    "\n",
    "因为问题a中卷积核函数的bias为0，因此，全连接层的biases也为0，即：\n",
    "\n",
    "```\n",
    "tensor([0., 0., 0.])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bea8a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "7. 多选题\n",
    "\n",
    "注：考试题中无选择题\n",
    "\n",
    "7.1 Which of the following techniques can be used to reduce model overfitting?\n",
    "\n",
    "    a. Data augmentation\n",
    "    b. Dropout\n",
    "    c. Batch Normalization\n",
    "    d. Using Adam instead of SGD\n",
    "    \n",
    "答：abc\n",
    "\n",
    "7.2 Which of the following is true about dropout?\n",
    "\n",
    "    a. Dropout leads to sparsity in the trained weights\n",
    "    b. At test time, dropout is applied with inverted keep probability\n",
    "    c. The larger the keep probability of a layer, the stronger the regularization of the weights in that layer\n",
    "    d. None of the above\n",
    "\n",
    "答：d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258bd51b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "8. 循环神经网络\n",
    "\n",
    "考虑循环神经网络中的通过时间反向传播，即backpropagation through time，BPTT。\n",
    "\n",
    "这里，仅考虑RNN的weights和各输入$x_t$均为scalars的情况。\n",
    "\n",
    "设RNN的每个时间步的隐状态和输出为：\n",
    "\n",
    "\\begin{align*}\n",
    "h_t &= w_{hx}x_t+w_{hh}h_{t-1},\\\\\n",
    "o_t &= w_{qh}h_t\n",
    "\\end{align*}\n",
    "\n",
    "设输入序列为$(x_1,x_2,\\dotsc,x_T)$，初始隐状态$h_0=0$，且损失函数为$L(o_T,y)$。\n",
    "\n",
    "请推导得到一般的$\\tfrac{\\partial L(o_T,y)}{\\partial w_{qh}}$，$\\tfrac{\\partial L(o_T,y)}{\\partial w_{hx}}$，以及$\\tfrac{\\partial L(o_T,y)}{\\partial w_{hh}}$的表达式。\n",
    "\n",
    "同时，结合[通过时间反向传播](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html)分析：在本问题中，什么时候可能会发生梯度爆炸，什么时候可能会发生梯度消失。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377104b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "答：\n",
    "\n",
    "$\\tfrac{\\partial L(o_T,y)}{\\partial w_{qh}}$为：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{\\partial L(o_T,y)}{\\partial w_{qh}}&=\\frac{\\partial L(o_T,y)}{\\partial o_T}\\frac{\\partial o_T}{\\partial w_{qh}}\\\\\n",
    "&=\\frac{\\partial L(o_T,y)}{\\partial o_T}h_T\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "$\\tfrac{\\partial L(o_T,y)}{\\partial w_{hx}}$为：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{\\partial L(o_T,y)}{\\partial w_{hx}}&=\\frac{\\partial L(o_T,y)}{\\partial o_T}\\frac{\\partial o_T}{\\partial h_T}\\frac{\\partial h_T}{\\partial w_{hx}}\\\\\n",
    "&=\\frac{\\partial L(o_T,y)}{\\partial o_T}w_{qh}\\biggl(x_T+w_{hh}\\frac{\\partial h_{T-1}}{\\partial w_{hx}}\\biggr)\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "而类似的，$\\tfrac{\\partial h_{T-1}}{\\partial w_{hx}}$为：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial h_{T-1}}{\\partial w_{hx}}=x_{T-1}+\\frac{\\partial h_{T-2}}{\\partial w_{hx}}\n",
    "\\end{equation*}\n",
    "\n",
    "代入并依次展开后得到：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{\\partial L(o_T,y)}{\\partial w_{hx}}&=\\frac{\\partial L(o_T,y)}{\\partial o_T}w_{qh}\\bigl(x_T+w_{hh}x_{T-1}+w^2_{hh}x_{T-2}+\\dotsb\\bigr)\\\\\n",
    "&=\\frac{\\partial L(o_T,y)}{\\partial o_T}w_{qh}\\sum_t w^t_{hh}x_{T-t}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "最后，$\\tfrac{\\partial L(o_T,y)}{\\partial w_{hh}}$为：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{\\partial L(o_T,y)}{\\partial w_{hh}}&=\\frac{\\partial L(o_T,y)}{\\partial o_T}\\frac{\\partial o_T}{\\partial h_T}\\frac{\\partial h_T}{\\partial w_{hh}}\\\\\n",
    "&=\\frac{\\partial L(o_T,y)}{\\partial o_T}\\frac{\\partial o_T}{\\partial h_T}\\biggl(h_{T-1}+w_{hh}\\frac{\\partial h_{T-1}}{\\partial w_{hh}}\\biggr)\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "而类似的，$\\tfrac{\\partial h_{T-1}}{\\partial w_{hh}}$为：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial h_{T-1}}{\\partial w_{hh}}=h_{T-2}+w_{hh}\\frac{\\partial h_{T-2}}{\\partial w_{hh}}\n",
    "\\end{equation*}\n",
    "\n",
    "代入并依次展开后得到：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{\\partial L(o_T,y)}{\\partial w_{hh}}&=\\frac{\\partial L(o_T,y)}{\\partial o_T}\\frac{\\partial o_T}{\\partial h_T}(h_{T-1}+w_{hh}h_{T-2}+w^2_{hh}h_{T-3}+\\dotsb)\\\\\n",
    "&=\\frac{\\partial L(o_T,y)}{\\partial o_T}\\frac{\\partial o_T}{\\partial h_T}\\sum_t w^t_{hh}h_{T-t-1}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "从$\\tfrac{\\partial L(o_T,y)}{\\partial w_{hx}}$，以及$\\tfrac{\\partial L(o_T,y)}{\\partial w_{hh}}$的表达式可以观察到，梯度的形式中有$w_{hh}$的潜在的非常大的幂。\n",
    "\n",
    "对于$w_{hh}<1$的情况来说，这将意味着梯度消失；对于$w_{hh}>1$的情况来说，这将意味着梯度爆炸。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c8ed1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "9. PyTorch的编程\n",
    "\n",
    "请利用PyTorch框架，编程实现Dropout、LayerNorm（仅针对全连接层的输出）、BatchNorm（仅针对全连接层的输出）。\n",
    "\n",
    "相关的函数接口如下：\n",
    "\n",
    "Dropout\n",
    "\n",
    "`def dropout(input, p=0.5, training=True)`\n",
    "\n",
    "LayerNorm\n",
    "\n",
    "`def layer_norm1d(input, gamma, beta, eps=1e-05)`\n",
    "\n",
    "BatchNorm\n",
    "\n",
    "`def batch_norm1d(input, gamma, beta, moving_mean, moving_var, eps=1e-05, momentum=0.1, training=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a5f06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "答：\n",
    "\n",
    "Dropout\n",
    "\n",
    "```\n",
    "def dropout(input, p=0.5, training=True):\n",
    "    if training or p == 0:\n",
    "        return X\n",
    "    if p == 1:\n",
    "        return torch.zeros_like(X)\n",
    "    mask = torch.rand(X.shape) > p\n",
    "    return mask.astype(torch.float32) * X / (1.0 - p)\n",
    "```\n",
    "\n",
    "LayerNorm\n",
    "\n",
    "```\n",
    "def layer_norm1d(input, gamma, beta, eps=1e-05):\n",
    "    mean = input.mean(-1, keepdim=True)\n",
    "    var = input.var(-1, keepdim=True)\n",
    "    return gamma * (x - mean) / (var + eps).sqrt() + beta\n",
    "```\n",
    "\n",
    "BatchNorm\n",
    "\n",
    "```\n",
    "def batch_norm1d(input, gamma, beta, moving_mean, moving_var, eps=1e-05, momentum=0.1, training=True):\n",
    "    if not training:\n",
    "        # 预测模式\n",
    "        input_hat = (input - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        mean = input.mean(dim=0)\n",
    "        var = input.var(dim=0)\n",
    "        \n",
    "        # 训练模式\n",
    "        input_hat = (input - mean) / (var + eps).sqrt()\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean.detach()\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var.detach()\n",
    "        \n",
    "    output = gamma * input_hat + beta\n",
    "    return output, moving_mean, moving_var\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
