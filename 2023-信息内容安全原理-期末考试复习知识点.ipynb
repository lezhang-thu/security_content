{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47df9cc",
   "metadata": {},
   "source": [
    "1. 你在设计一个卷积神经网络。对于每一层，请计算weights的个数，biases的个数，以及相应的特征图的大小。\n",
    "   \n",
    "   相关符号的含义如下：\n",
    "   * CONV-K-N表示卷积核的个数为N，每个卷积核的大小为KxK，填充和步幅分别为0和1；\n",
    "   * POOL-K表示一个KxK的池化层，步幅为K，填充为0；\n",
    "   * FC-N表示一个全连接层，有N个神经元。\n",
    "       \n",
    "| Layer     | Activation map dimensions  | Number of weights  | Number of biases  |\n",
    "| :----:    |  :-----------------------: | :----------------: | :--------------:  |\n",
    "| INPUT     |  128x128x3                 |         0          |           0       |\n",
    "| CONV-9-32 |                            |                    |                   |\n",
    "| POOL-2    |                            |                    |                   |\n",
    "| CONV-5-64 |                            |                    |                   |\n",
    "| POOL-2    |                            |                    |                   |\n",
    "| CONV-5-64 |                            |                    |                   |\n",
    "| POOL-2    |                            |                    |                   |\n",
    "| FC-3      |                            |                    |                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae4b35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. 逻辑回归 / Logistic Regression\n",
    "\n",
    "给定如下的逻辑回归，其中$(x_1,x_2)$为输入，$y$为相应的标签值（为0或者1）。\n",
    "\n",
    "计算过程如下：\n",
    "\n",
    "\\begin{align*}\n",
    "z&=w_1x_1+w_2x_2+b\\\\\n",
    "a&=\\sigma(z)\n",
    "\\end{align*}\n",
    "\n",
    "其中，$\\sigma(z)=\\tfrac{1}{1+e^{-z}}$，同时，$L(a,y)=-(y\\log a+(1-y)\\log (1-a))$，其中，$\\log$以自然底数$e$为底。\n",
    "\n",
    "请回答如下的问题：\n",
    "\n",
    "a. 请根据链式法则，给出$\\tfrac{dL(a,y)}{dz}$的计算过程；\n",
    "\n",
    "b. 若采取的是随机梯度下降算法，且学习率为1e-4，那么，在$(w_1,w_2,b)$为$(0.5,-0.5)$时，若$(x_1,x_2,y)$为$(1,1,0)$，则执行一次梯度下降迭代后，$(w_1,w_2,b)$为多少。请给出计算过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936041a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. 给定一个63x63x16的输入，并使用大小为7x7的32个卷积核进行卷积（步幅为2和无填充），请问输出是多少，为什么。\n",
    "\n",
    "4. 给定输入的张量，形状为64x64x16，请问作用于其上的单个1x1的卷积核含有多少个参数（包括bias），为什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d3263",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "5. 均方损失\n",
    "\n",
    "给定如下的计算过程，其中$(x_1,x_2)$为输入，$y$为目标值：\n",
    "\n",
    "\\begin{align*}\n",
    "z_1&=w_{11}x_1+w_{12}x_2+b_1\\\\\n",
    "z_2&=w_{21}x_1+w_{22}x_2+b_2\\\\\n",
    "a_1&=\\mathrm{ReLU}(z_1)\\\\\n",
    "a_2&=\\mathrm{ReLU}(z_2)\\\\\n",
    "a&=a_1w_{31}+x_1w_{32}\n",
    "\\end{align*}\n",
    "\n",
    "而损失函数为均方损失，也就是说，$L(a,y)=\\tfrac{1}{2}(a-y)^2$。\n",
    "\n",
    "若采取的是随机梯度下降算法，且学习率为1e-4，那么，在$b_1=b_2=0$，$w_{11}=w_{12}=-1$，$w_{21}=w_{22}=1$，$w_{31}=w_{32}=1$时，若$(x_1,x_2,y)$为$(1,1,0.5)$，则执行一次梯度下降迭代后，$w_{21}$为多少。请给出计算过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e373438a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "6. 给定输入tensor `x`：\n",
    "\n",
    "```\n",
    "tensor([[ 0,  1,  2,  3],\n",
    "        [ 4,  5,  6,  7]])\n",
    "```\n",
    "\n",
    "考虑卷积核`K`（注意bias为0）：\n",
    "\n",
    "```\n",
    "tensor([[-1.,  1.],\n",
    "        [ 1.,  1.]])\n",
    "```\n",
    "\n",
    "a. 若stride为1，padding为0，那么，输出的tensor应该是什么？\n",
    "\n",
    "b. 现在需要你用全连接层去替换掉上述卷积层，并得到和问题a相同的输出tensor。\n",
    "\n",
    "那么，请问全连接层的weights和biases分别是什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bea8a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "7. 多选题\n",
    "\n",
    "注：考试题中无选择题\n",
    "\n",
    "7.1 Which of the following techniques can be used to reduce model overfitting?\n",
    "\n",
    "    a. Data augmentation\n",
    "    b. Dropout\n",
    "    c. Batch Normalization\n",
    "    d. Using Adam instead of SGD\n",
    "    \n",
    "7.2 Which of the following is true about dropout?\n",
    "\n",
    "    a. Dropout leads to sparsity in the trained weights\n",
    "    b. At test time, dropout is applied with inverted keep probability\n",
    "    c. The larger the keep probability of a layer, the stronger the regularization of the weights in that layer\n",
    "    d. None of the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258bd51b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "8. 循环神经网络\n",
    "\n",
    "考虑循环神经网络中的通过时间反向传播，即backpropagation through time，BPTT。\n",
    "\n",
    "这里，仅考虑RNN的weights和各输入$x_t$均为scalars的情况。\n",
    "\n",
    "设RNN的每个时间步的隐状态和输出为：\n",
    "\n",
    "\\begin{align*}\n",
    "h_t &= w_{hx}x_t+w_{hh}h_{t-1},\\\\\n",
    "o_t &= w_{qh}h_t\n",
    "\\end{align*}\n",
    "\n",
    "设输入序列为$(x_1,x_2,\\dotsc,x_T)$，初始隐状态$h_0=0$，且损失函数为$L(o_T,y)$。\n",
    "\n",
    "请推导得到一般的$\\tfrac{\\partial L(o_T,y)}{\\partial w_{qh}}$，$\\tfrac{\\partial L(o_T,y)}{\\partial w_{hx}}$，以及$\\tfrac{\\partial L(o_T,y)}{\\partial w_{hh}}$的表达式。\n",
    "\n",
    "同时，结合[通过时间反向传播](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html)分析：在本问题中，什么时候可能会发生梯度爆炸，什么时候可能会发生梯度消失。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c8ed1",
   "metadata": {},
   "source": [
    "9. PyTorch的编程\n",
    "\n",
    "请利用PyTorch框架，编程实现Dropout、LayerNorm（仅针对全连接层的输出）、BatchNorm（仅针对全连接层的输出）。\n",
    "\n",
    "相关的函数接口如下：\n",
    "\n",
    "Dropout\n",
    "\n",
    "`def dropout(input, p=0.5, training=True)`\n",
    "\n",
    "LayerNorm\n",
    "\n",
    "`def layer_norm1d(input, gamme, beta, eps=1e-05)`\n",
    "\n",
    "BatchNorm\n",
    "\n",
    "`def batch_norm1d(input, gamma, beta, moving_mean, moving_var, eps=1e-05, momentum=0.1, training=True)`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
